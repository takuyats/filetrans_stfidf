{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@word クラス\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import scipy\n",
    "#ライブラリインポート\n",
    "#基本ライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#形態素解析\n",
    "import MeCab\n",
    "#モデル保存\n",
    "#from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StfidfVectorizer(TfidfVectorizer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus:スペース区切りのコーパス\n",
    "    model：word2vecのmodel\n",
    "    n：？√nでword2vecの分散表現上での類似語上位\n",
    "    epoch：スコア更新回数\n",
    "    word:更新対象の言葉   \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,corpus,model,n,epoch,token_pattern,ngram_range):\n",
    "        TfidfVectorizer.__init__(self)\n",
    "        self.token_pattern=token_pattern\n",
    "        self.ngram_range = ngram_range\n",
    "        self._corpus = corpus\n",
    "        self._model = model\n",
    "        self._n = n\n",
    "        self._epoch = epoch\n",
    "\n",
    "    def update_score(self,word):\n",
    "        X = self.fit_transform(self._corpus)\n",
    "        wds = self.get_feature_names()        \n",
    "        S = {}\n",
    "        for wd in wds:\n",
    "            #print(wd)\n",
    "            S[wd] = X[0, self.vocabulary_[wd]] * 10.0\n",
    "        print(\"「 {} 」の初期スコア：{}\".format(word,S[word]))\n",
    "\n",
    "        for ep in range(1,self._epoch):\n",
    "            prev_S = S[word]\n",
    "            weight_e = self.relevant_word(word,prev_S)\n",
    "            S[word] = prev_S * 1/(\n",
    "                             1+np.linalg.norm(self.e(wd))*np.linalg.norm(weight_e)*scipy.spatial.distance.cosine(self.e(word), weight_e)\n",
    "                             )\n",
    "        print(\"「 {}　」の最終スコア：{}\".format(word,S[word]))\n",
    "\n",
    "\n",
    "    def relevant_word(self,wd,prev_S):\n",
    "        root_n = math.floor(math.sqrt(self._n))\n",
    "        weightened_expected_value = 0 \n",
    "        for i in range(1,int(root_n)+1):\n",
    "            weight=\\\n",
    "            1/(1-self.all_weight_score(wd,root_n,prev_S))\\\n",
    "            *self.e(wd)\n",
    "            weightened_expected_value += weight\n",
    "        return weightened_expected_value/root_n\n",
    "\n",
    "\n",
    "    def all_weight_score(self,wd,root_n,prev_S):\n",
    "        try:\n",
    "            results = self._model.wv.most_similar(positive=[wd])\n",
    "            words =[w[0] for w in results]\n",
    "            return prev_S/self.score_sum(prev_S,words)\n",
    "        except KeyError:\n",
    "            print (\"not in vocabulary\")\n",
    "            return 0\n",
    "\n",
    "    def e(self,wd):\n",
    "        try:\n",
    "            return model.wv[wd]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def score_sum(self,prev_S,words):\n",
    "        scoresum = 0\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            try:\n",
    "                scoresum += S[word]\n",
    "            except:\n",
    "                pass\n",
    "        return scoresum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習データ読み込み\n",
    "df = pd.read_csv(\"../tele2/theme/Train_変復調・符号変換.tsv\",sep = '\\t')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価データ読み込み\n",
    "df_test = pd.read_csv(\"../tele2/theme/Eval_変復調・符号変換.tsv\",sep = '\\t')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EOS\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #形態素解析\n",
    "mecabTagger = MeCab.Tagger(\"-Ochasen\")\n",
    "mecabTagger.parse('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#形態素解析を実施して、'名詞'の単語を抽出し、半角スペースで区切り１行にする関数\n",
    "def get_surface(text):\n",
    "    result = []\n",
    "    #ストップワードリスト\n",
    "    #名詞\n",
    "    stopnounslist=[\"図\",\"前記\",\"上記\",\"もの\",\"こと\",\"ため\",\"よう\",\"それぞれ\",\"さ\",\"ａ\",\"装置\"]\n",
    "    #動詞\n",
    "    stopverbslist=[\"する\",\"られる\",\"れる\",\"なる\",\"できる\",\"せる\",\"ある\",\"いる\",\"おる\"]\n",
    "    #区切り文字\n",
    "    stopleftonelist=[\"(\",\")\",\"[\"\"]\",\"{\",\"}\",\"（\",\"）\",\"［\",\"］\",\"｛\",\"｝\",\"「\",\"」\",\"〔\",\"〕\",\"『\",\"』\",\"｢\",\"｣\",\"【\",\"】\",\"＜\",\"＞\",\"<\",\">\",\"%\",\"+\",\"-\",\"/\",\"'\",\"\\\"\",\"`\",\"*\",\":\",\";\",\".\",\"，\",\",\",\"。\",\"｡\",\"÷\",\"？\",\"｜\",\"|\"]\n",
    "\n",
    "    node = mecabTagger.parseToNode(text)\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] in('名詞') and node.feature.split(\",\")[1] in('一般') and node.surface.isdigit() == False and node.surface not in(stopnounslist) and node.surface not in(stopleftonelist):\n",
    "            result.append(node.surface)\n",
    "        #elif node.feature.split(\",\")[0] in('動詞') and node.feature.split(\",\")[6] not in(stopleftonelist) and node.feature.split(\",\")[6] not in(stopverbslist):\n",
    "            #result.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semantic_06261/anaconda3/envs/my_env/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '学習データ_形態素解析.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-2d6320fe6ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#print(df['text_tokenized'][601:611])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#print(df['text_tokenized'][801:811])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'学習データ_形態素解析.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4683\u001b[0m             )\n\u001b[1;32m   4684\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4687\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"isna\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         )\n\u001b[0;32m-> 3228\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '学習データ_形態素解析.csv'"
     ]
    }
   ],
   "source": [
    "#学習データ用コーパス作成\n",
    "text_tokenized = []\n",
    "for text in df['text']:\n",
    "    text_tokenized.append(get_surface(text))\n",
    "\n",
    "df['text_tokenized'] = text_tokenized\n",
    "\n",
    "#print(df['text_tokenized'][1:11])\n",
    "#print(df['text_tokenized'][201:211])\n",
    "#print(df['text_tokenized'][401:411])\n",
    "#print(df['text_tokenized'][601:611])\n",
    "#print(df['text_tokenized'][801:811])\n",
    "#df['text_tokenized'].to_csv('学習データ_形態素解析.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semantic_06261/anaconda3/envs/my_env/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#評価データ用コーパス作成\n",
    "text_tokenized = []\n",
    "for text in df_test['text']:\n",
    "    text_tokenized.append(get_surface(text))\n",
    "\n",
    "df_test['text_tokenized'] = text_tokenized\n",
    "#print(df['text_tokenized'][1:11])\n",
    "#print(df['text_tokenized'][201:211])\n",
    "#print(df['text_tokenized'][401:411])\n",
    "#print(df['text_tokenized'][601:611])\n",
    "#print(df['text_tokenized'][801:811])\n",
    "#df_test['text_tokenized'].to_csv('評価データ_形態素解析.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDFで学習用データ、評価用データ作成\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X       = vectorizer.fit_transform(df['text_tokenized'])    #学習用データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X       = pd.DataFrame(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1497 entries, 0 to 1496\n",
      "Columns: 23168 entries, 0 to 23167\n",
      "dtypes: float64(23168)\n",
      "memory usage: 264.6 MB\n"
     ]
    }
   ],
   "source": [
    "#次元数確認\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5J064\n",
       "507     5J065\n",
       "657     5K004\n",
       "920     5K022\n",
       "1292    5K159\n",
       "Name: theme, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ラベル確認\n",
    "df['theme'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ブ－スティング用正解ラベル作成\n",
    "df.loc[df['theme'] == '5J064', 'theme1'] = 1\n",
    "df.loc[df['theme'] == '5J065', 'theme1'] = 2\n",
    "df.loc[df['theme'] == '5K004', 'theme1'] = 3\n",
    "df.loc[df['theme'] == '5K022', 'theme1'] = 4\n",
    "df.loc[df['theme'] == '5K159', 'theme1'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#型変換\n",
    "df['theme1'] = df['theme1'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lightGBM\n",
    "import lightgbm\n",
    "clf_lgb = lightgbm.LGBMClassifier()\n",
    "clf_lgb.fit(X, df['theme1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def make_corpus(text):\n",
    "#コーパスはテストデータの方がよい？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "epoch = 5\n",
    "word = \"符号\"\n",
    "#corpus = X\n",
    "corpus = df['text_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for text in df['text_tokenized']:\n",
    "    #text_list = text.split(' ')\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vecによる仮モデル\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbmによるモデル\n",
    "model = clf_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「 ハードディスク 」の初期スコア：0.19505742956259792\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "「 ハードディスク　」の最終スコア：nan\n",
      "「 メモリ 」の初期スコア：0.0\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "「 メモリ　」の最終スコア：nan\n",
      "「 データ 」の初期スコア：0.17528167482471682\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "「 データ　」の最終スコア：nan\n",
      "「 双方 」の初期スコア：0.0\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "「 双方　」の最終スコア：nan\n",
      "「 アドレス 」の初期スコア：0.0\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "not in vocabulary\n",
      "「 アドレス　」の最終スコア：nan\n"
     ]
    }
   ],
   "source": [
    "stf = StfidfVectorizer(corpus,model,n,epoch,token_pattern='(?u)\\\\b\\\\w+\\\\b',ngram_range=(1,1))\n",
    "#words=[\"feed\",\"a\",\"the\",\"compact\",\"stowage\",\"and\",\"deployment\"]\n",
    "words=[\"ハードディスク\",\"メモリ\",\"データ\",\"双方\",\"アドレス\"]\n",
    "for word in words:\n",
    "    stf.update_score(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
